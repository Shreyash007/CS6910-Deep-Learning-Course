{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEM+vzGmyz0MCZxqPRDKsn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyash007/CS6910-Deep-Learning-Course/blob/main/Assignment1(Q1_Q3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 1 CS6910**\n",
        "Shreyash Gadgil (ED22S016)"
      ],
      "metadata": {
        "id": "9LINEVNo0Fo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login aa5afea12b4fda1e7f8310b597eb17c73d1176d2 #my API key for wandb login "
      ],
      "metadata": {
        "id": "sC2z2tS8Ccxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Libraries used**\n",
        "1. numpy: For matrix calculations\n",
        "2. keras: For importing required dataset\n",
        "3. wandb: For logging the data\n",
        "4. tqdm: to create progress bar for loops\n",
        "5. sklearn: For random test-train split "
      ],
      "metadata": {
        "id": "G4iCjM7Q0SiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0c3goK0vrQL"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1 Solution**\n"
      ],
      "metadata": {
        "id": "a0Q5Mjvy0XZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "#importing dataset(add this in show_images function as variable)\n",
        "\n",
        "#checking size of training and test dataset\n",
        "print(\"Training dataset shape, X=\",X_train.shape,\", Y=\",Y_train.shape)\n",
        "print(\"Test dataset shape, X=\",X_test.shape,\", Y=\",Y_test.shape)\n",
        "\n",
        "n_img_classes=len(np.unique(Y_train)) #to find out number of unique images \n",
        "img_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] #taken from keras documentation\n",
        "\n",
        "\n",
        "def show_images(n_img_classes,img_labels,X,Y):\n",
        "\n",
        "   setImgs = []\n",
        "   setLabels=[]\n",
        "\n",
        "   for i in range(len(X)):\n",
        "     if len(setImgs)==n_img_classes:\n",
        "       break\n",
        "     if img_labels[Y[i]] not in setLabels:\n",
        "      setImgs.append(X[i])\n",
        "      setLabels.append(img_labels[Y[i]])\n",
        "\n",
        "   run=wandb.init(project='CS-6910 A1',entity='shreyashgadgil007',reinit=True)\n",
        "   run.log({\"Sample from each class\":list(wandb.Image(img,caption= caption) for img,caption in zip(setImgs,setLabels))})\n",
        "   run.finish()\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#--------------------------------------------------------------------------Q1 SOLUTION---------------------------------------------------------------------------------------------\n",
        "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#UNCOMMENT BELOW FUNCTION TO GET Q1 SOLUTION\n",
        "#show_images(n_img_classes,img_labels,X_test,Y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEPtktkP0cLG",
        "outputId": "ee39b168-d63c-4290-956e-e2c25395fbb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Training dataset shape, X= (60000, 28, 28) , Y= (60000,)\n",
            "Test dataset shape, X= (10000, 28, 28) , Y= (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing dataset for operations**\n",
        "1. Creating random training,test,validation split for dataset\n",
        "2. Reshaping image matrix and normalizing pixel values\n",
        "3. One hot encoding classifications\n"
      ],
      "metadata": {
        "id": "ySDC6Ck0GjBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_preprocess():\n",
        "  #change the code below to accept different dataset\n",
        "  (X_1, Y_1), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "  \n",
        "  #importing dataset again and normalizing\n",
        "  X_1 = X_1.reshape(X_1.shape[0],-1)/255.0\n",
        "  X_test = X_test.reshape(X_test.shape[0],-1)/255.0\n",
        "\n",
        "  #training and validation split as specified in the question 10%\n",
        "  X_train, X_val, Y_train, Y_val= train_test_split(X_1,Y_1,test_size=0.1,random_state=0)\n",
        "  \n",
        "  #one hot encoding\n",
        "  Y_train_encoded=np.zeros((Y_train.shape[0],10))\n",
        "  for i in range(len(Y_train)):\n",
        "    Y_train_encoded[i][Y_train[i]]=1\n",
        "\n",
        "  Y_val_encoded=np.zeros((Y_val.shape[0],10))\n",
        "  for i in range(len(Y_val)):\n",
        "    Y_val_encoded[i][Y_val[i]]=1\n",
        "\n",
        "  Y_test_encoded=np.zeros((Y_test.shape[0],10))\n",
        "  for i in range(len(Y_test)):\n",
        "    Y_test_encoded[i][Y_test[i]]=1\n",
        "\n",
        "  return X_train.T,X_test.T,X_val.T,Y_train.T,Y_val.T,Y_test.T,Y_train_encoded.T,Y_val_encoded.T,Y_test_encoded.T\n",
        "\n",
        "#Below variables will be used in the entire code\n",
        "X_train,X_test,X_val,Y_train,Y_val,Y_test,Y_train_encoded,Y_val_encoded,Y_test_encoded=dataset_preprocess()\n",
        "\n",
        "print(Y_val_encoded.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train_encoded.shape)\n",
        "print(X_train.shape)\n",
        "print(Y_val.shape)\n",
        "print(Y_test_encoded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuMSSFIfGiZ_",
        "outputId": "e28cff69-531e-4a2f-d0ec-c69de20bb9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 6000)\n",
            "(784, 10000)\n",
            "(10, 54000)\n",
            "(784, 54000)\n",
            "(6000,)\n",
            "(10, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper functions**\n",
        "1. Declared activation functions and their gradients\n",
        "2. Declared loss functions and their gradients\n",
        "3. Declared initialisation functions \n",
        "4. Declared a function to initialize weights and biases"
      ],
      "metadata": {
        "id": "XCmVKBiE362R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------------------ACTIVATION FUNCTIONS AND THEIR GRADIENTS-------------------------------------------------------------------------------\n",
        "def relu(X):\n",
        "  return np.maximum(0,X)\n",
        "\n",
        "def grad_relu(X):\n",
        "  return X>0\n",
        "\n",
        "def sigmoid(X):\n",
        "  return 1/(1+np.exp(-X))\n",
        "\n",
        "def grad_sigmoid(X):\n",
        "  return (sigmoid(X))*(1-sigmoid(X))\n",
        "\n",
        "def tanh(X):\n",
        "  return (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
        "\n",
        "def grad_tanh(X):\n",
        "  return 1-(tanh(X))**2\n",
        "\n",
        "def softmax(X):\n",
        "  e_X = np.exp(X - np.max(X, axis = 0))\n",
        "  return e_X / e_X.sum(axis = 0)\n",
        "\n",
        "\n",
        "activation_function={\n",
        "      'sigmoid':sigmoid,\n",
        "      'relu':relu,\n",
        "      'tanh':tanh\n",
        "}\n",
        "\n",
        "grad_activation_function={\n",
        "      'sigmoid':grad_sigmoid,\n",
        "      'relu':grad_relu,\n",
        "      'tanh':grad_tanh\n",
        "}\n",
        "#---------------------------------------------------------------------------LOSS FUNCTION---------------------------------------------------------------------------------------------\n",
        "def cross_entropy_loss(Y_encoded,Y,Y_pred,lambd,b):\n",
        " loss = (-np.sum(np.multiply(Y_encoded,np.log(Y_pred)))+((lambd/2.)*b))/Y.shape[0]\n",
        " return loss\n",
        " \n",
        "def squared_error_loss(Y_encoded,Y,Y_pred,lambd,b):\n",
        "  loss=((1/2)*(np.sum(Y_encoded-Y_pred))**2)/Y.shape[0]+(lambd*b*0)\n",
        "  return loss\n",
        "\n",
        "loss_function={\n",
        "       'cross_entropy':cross_entropy_loss,\n",
        "       'square_loss':squared_error_loss\n",
        "}   \n",
        "\n",
        "\n",
        "def grad_cross_entropy(Y,Y_pred):\n",
        "  return (Y_pred-Y)\n",
        "\n",
        "def grad_squared_error_loss(Y,Y_pred):  \n",
        "  return (Y_pred-Y)*(Y_pred)*(1-Y_pred)\n",
        "\n",
        "grad_loss_function={\n",
        "    'cross_entropy':grad_cross_entropy,\n",
        "    'square_loss':grad_squared_error_loss\n",
        "}\n",
        "\n",
        "#----------------------------------------------------------------------INITIALISATION FUNCTIONS---------------------------------------------------------------------------------------\n",
        "def random_initialisation(shape):\n",
        "  # Initialising a random matrix with given dimensions (shape) as tuple\n",
        "  np.random.seed(0)\n",
        "  return np.random.randn(*shape)*0.5 #multiplied by 0.5 to have smaller values, to have better initialisation weights\n",
        "\n",
        "def xavier_initialisation(shape):\n",
        "    # Calculate the number of inputs and outputs\n",
        "    n_in = shape[0]\n",
        "    n_out = shape[1]    \n",
        "    # Calculate the variance of the normal distribution\n",
        "    variance = 2.0 / (n_in + n_out)\n",
        "    # Initialize the weights with random values drawn from the normal distribution\n",
        "    np.random.seed(0)\n",
        "    weights = np.random.randn(n_in, n_out) * np.sqrt(variance)*2.0\n",
        "    \n",
        "    return weights\n",
        "\n",
        "initialisation_function={\n",
        "    'random':random_initialisation,\n",
        "    'xavier':xavier_initialisation\n",
        "}\n",
        "#------------------------------------------------------------------INITIALIZING WEIGHTS AND BIASES------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def initialize_w_b(input_layer,hidden_layer,output_layer,init):\n",
        "  weights=[]\n",
        "  biases=[]\n",
        "  layers=[input_layer]+hidden_layer+[output_layer]\n",
        "  for i in range(len(hidden_layer)+1): \n",
        "    weights.append(initialisation_function[init]((layers[i+1],layers[i])))\n",
        "    biases.append(np.random.randn(layers[i+1],1))\n",
        "  return weights, biases\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "SIulf2iI377n",
        "outputId": "0c49bdf8-aae5-49ed-bbea-5d5ba0fddcab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#testing weights and biases output(for debugging shape size of matrix) \\noutput_layer = 10\\ninput_layer = X_test.shape[1]\\nprint(X_test.shape)\\nhidden_layer = [64, 64]\\nweights, biases = initialize_w_b(input_layer, hidden_layer, output_layer, 'xavier')\\nfor i in range(len(weights)):\\n    print(weights[i].shape)\\n    \\nfor i in range(len(biases)):\\n    print(biases[i].shape)\\n\\nweights2, biases2 = initialize_w_b(input_layer, hidden_layer, output_layer, 'random')\\nfor i in range(len(weights2)):\\n    print(weights2[i].shape)\\n    \\nfor i in range(len(biases2)):\\n    print(biases2[i].shape)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network**\n",
        "1. With forward propagation\n",
        "2. Backward propagation with different optimizers\n",
        "3. A train function, which fits and trains the given set\n",
        "\n",
        "# **Q3 IMPLEMENTED BELOW**"
      ],
      "metadata": {
        "id": "L20aDODvd-I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet():\n",
        "\n",
        "  def __init__(self,input_layer,hidden_layer,output_layer,initialisation_func,act_function,loss_func):\n",
        "     self.input_layer=input_layer\n",
        "     self.hidden_layer=hidden_layer\n",
        "     self.output_layer=output_layer\n",
        "     self.initialisation_func=initialisation_func\n",
        "     self.act_function=act_function\n",
        "     self.loss_func=loss_func\n",
        "     self.weights,self.biases = initialize_w_b(self.input_layer,self.hidden_layer,self.output_layer,self.initialisation_func)\n",
        "     self.layer_size=len(self.hidden_layer)\n",
        "\n",
        "  \n",
        "  def forward_propagation(self,X):\n",
        "     #pre-activation\n",
        "     self.a=[]\n",
        "     #post-activation\n",
        "     self.h=[]\n",
        "     l=0\n",
        "     \n",
        "     #pre-activation and post-activation for input layer and first hidden layer\n",
        "     self.a.append((self.weights[l]@X)+self.biases[l])#WX+b\n",
        "     self.h.append(activation_function[self.act_function](self.a[l]))\n",
        "     \n",
        "     #pre-activation and post-activation between hidden layers\n",
        "     for l in range(1,self.layer_size):\n",
        "       self.a.append((self.weights[l]@self.h[l-1])+self.biases[l])\n",
        "       self.h.append(activation_function[self.act_function](self.a[l]))\n",
        "       \n",
        "     #pre-activation and post-activation between last hidden layer and output layer\n",
        "     l=self.layer_size \n",
        "     self.a.append((self.weights[l]@self.h[l-1])+self.biases[l])\n",
        "     self.h.append(softmax(self.a[l]))\n",
        "     \n",
        "     return self.h[-1]\n",
        "\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#---------------------------------------------------Q3 BACK PROPPAGATION FRAMEWORK WITH OPTIMIZATION FUNCTIONS-----------------------------------------------------------------\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
        "  def back_propagation(self,X,Y):\n",
        "\n",
        "        g_a  = [0]*(self.layer_size+1)\n",
        "        g_h  = [0]*(self.layer_size+1)\n",
        "        g_w  = [0]*(len(self.weights))\n",
        "        g_b  = [0]*(len(self.biases))\n",
        "        batch_size = X.shape[1] \n",
        "\n",
        "        for k in reversed(range(self.layer_size+1)):\n",
        "            #calculating loss function gradients for weights and biases at output\n",
        "            if k == self.layer_size:\n",
        "              g_a[k]=grad_loss_function[self.loss_func](Y,self.h[k])\n",
        "            \n",
        "            #calculating gradients for hidden layers     \n",
        "            else:\n",
        "                g_h[k] = (1/batch_size)*((self.weights[k+1].T)@(g_a[k+1]))\n",
        "                g_a[k] = (1/batch_size)*((g_h[k])*(grad_activation_function[self.act_function](self.a[k])))#Here we use '*' operator for elementwise matrix multiplication\n",
        "                \n",
        "            #calculating gradients of weights \n",
        "            if k == 0:\n",
        "                g_w[k] = (1/batch_size)*((g_a[k])@(X.T)) \n",
        "            else:\n",
        "                g_w[k] = (1/batch_size)*((g_a[k])@(self.h[k-1].T))\n",
        "            \n",
        "            #calculating gradients of biases\n",
        "            g_b[k]  = (1/batch_size)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "        return g_w,g_b\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#-----------------------------------------------------STOCHASTIC GRADIENT DESCENT AND OTHER OPTIMIZERS-------------------------------------------------------------------------------\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
        "  def stochastic_gradient_descent(self,W,b,g_w,g_b,lr,lambd):\n",
        "      #Weights=Weights-(learning rate)*(grad weights)-(learning rate*lambda)*(Weights)(this is weight decay with L2 regularization)\n",
        "      W=W-np.multiply(lr,g_w)-np.multiply(lr*lambd,W)\n",
        "\n",
        "      #biases=biases-(learning rate)*(grad biases)\n",
        "      b=b-np.multiply(lr,g_b)\n",
        "      return W,b\n",
        "  \n",
        "  def momentum_based_gradient_descent(self,W,b,g_w,g_b, u_w_i, u_b_i, lr, lambd, gamma):\n",
        "      #u_t=(beta)*(u_t-1)+(grad weight)[u_0 is initialised as zero]\n",
        "      u_w=np.multiply(gamma,u_w_i)+g_w\n",
        "      #weights=weights-(lr)*(updated_weights)-(weight decay L2 regularization)\n",
        "      W=W-np.multiply(lr,u_w)-np.multiply(lr*lambd,W)\n",
        "      \n",
        "      #similarly for biases but without weight decay term\n",
        "      u_b=np.multiply(gamma,u_b_i)+g_b\n",
        "      b=b-np.multiply(lr,u_b)\n",
        "      return W,b,u_w,u_b\n",
        "  \n",
        "  def nesterov_accelerated_gradient_descent(self,W,b,g_w,g_b,lr,lambd,gamma,u_w_i, u_b_i,b_input,b_output):\n",
        "      w_t=W\n",
        "      b_t=b\n",
        "      #here we make changes to global variables as we need to update the weights(look ahead) for calculating gradients\n",
        "      self.weights = self.weights - np.multiply(gamma,u_w_i)\n",
        "      self.biases = self.biases - np.multiply(gamma,u_b_i)\n",
        "      output =  self.forward_propagation(b_input)\n",
        "      #in this step, gradients are recalculated in global variables with updated weight values \n",
        "      g_weights,g_biases = self.back_propagation(b_input,b_output)\n",
        "\n",
        "      #u_t=(gamma)*(u_(t-1))+gradient of(w_t-(gamma)*(u_(t-1)))\n",
        "      u_w=np.multiply(gamma,u_w_i)+g_weights\n",
        "      #weights=weights-(lr)*(updated_weights)-(weight decay L2 regularization)\n",
        "      w_t = w_t - u_w - np.multiply(lr*lambd,w_t)\n",
        "\n",
        "      #similarly for biases but without weight decay term\n",
        "      u_b=np.multiply(gamma,u_b_i)+g_b\n",
        "      b=b-np.multiply(lr,u_b) \n",
        "\n",
        "      return w_t,b,u_w,u_b\n",
        "  \n",
        "  def rmsprop(self,W,b,g_w,g_b,lr,lambd,beta,eps,vw,vb):\n",
        "      #V_t= (beta)*(V_t-1)+(1-beta)*(grad weight)^2 \n",
        "      vw = np.multiply(vw,beta) + np.multiply(1-beta,np.power(g_w,2))           \n",
        "      #similarly for biases\n",
        "      vb = np.multiply(vb,beta) + np.multiply(1-beta,np.power(g_b,2))\n",
        "      \n",
        "      #w_t= (w_t-1)-((lr)/(V_t+eps)^(1/2))*(grad weight)-(L2 regularization weight decay)\n",
        "      W = W - np.multiply(g_w,lr/np.power(vw+eps,1/2))- np.multiply(lr*lambd,W)\n",
        "      #similarly for biases\n",
        "      b = b - np.multiply(g_b,lr/np.power(vb+eps,1/2))\n",
        "      return W,b,vw,vb\n",
        "  \n",
        "  def adam(self,W,b,g_w,g_b,beta1,beta2,lr ,m_t_i ,v_t_i ,m_b_i ,v_b_i,eps,i,lambd):\n",
        "      \n",
        "      m_t = np.multiply(beta1,m_t_i) + np.multiply(1-beta1,g_w)\n",
        "      v_t = np.multiply(beta2,v_t_i) + np.multiply(1-beta2,np.power(g_w,2))\n",
        "      m_b = np.multiply(beta1,m_b_i) + np.multiply(1-beta1,g_b)\n",
        "      v_b = np.multiply(beta2,v_b_i) + np.multiply(1-beta2,np.power(g_b,2))\n",
        "                \n",
        "      #normalization of moment          \n",
        "      m_hat_w = m_t/(1 - np.power(beta1,i+1))\n",
        "      m_hat_b = m_b/(1 - np.power(beta1,i+1))\n",
        "\n",
        "      #normalization          \n",
        "      v_hat_w = v_t/(1 - np.power(beta2,i+1))\n",
        "      v_hat_b = v_b/(1 - np.power(beta2,i+1))\n",
        "      \n",
        "      W = W - ((lr / np.power(v_hat_w + eps, 1/2)) * m_hat_w) - np.multiply(lr*lambd,W)\n",
        "      \n",
        "      b = b - ((lr / np.power(v_hat_b + eps, 1/2)) * m_hat_b)\n",
        "      return W,b,m_t,v_t,m_b,v_b\n",
        "  \n",
        "  def nadam(self,W,b,g_w,g_b,beta1,beta2,lr ,m_t_i ,v_t_i ,m_b_i ,v_b_i,eps,i,lambd):\n",
        "      m_t =  np.multiply(beta1,m_t_i) + np.multiply(1 - beta1,g_w)\n",
        "      v_t =  np.multiply(beta2,v_t_i) + np.multiply(1 - beta2,np.power(g_w, 2))\n",
        "\n",
        "      m_b =  np.multiply(beta1,m_b_i) + np.multiply(1 - beta1,g_b)\n",
        "      v_b =  np.multiply(beta2,v_b_i) + np.multiply(1 - beta2,np.power(g_b, 2))\n",
        "                \n",
        "      m_hat_w = m_t / (1 - np.power(beta1, i+1)) \n",
        "      v_hat_t = v_t / (1 - np.power(beta2, i+1))\n",
        "\n",
        "      m_hat_b = m_b / (1 - np.power(beta1, i+1)) \n",
        "      v_hat_b = v_b / (1 - np.power(beta2, i+1))\n",
        "  \n",
        "      a1 = (1-beta1)/(1-np.power(beta1,i+1))\n",
        "      update_w = np.multiply(lr/(np.power(v_hat_t + eps,1/2)),(np.multiply(a1,g_w) + np.multiply(beta1,m_hat_w)))\n",
        "      update_b = np.multiply(lr/(np.power(v_hat_b + eps,1/2)),(np.multiply(a1,g_b)+np.multiply(beta1,m_hat_b) ))\n",
        "      W = W - update_w - np.multiply(lr*lambd,W)\n",
        "      b = b - update_b    \n",
        "      return W,b,m_t,v_t,m_b,v_b\n",
        "  \n",
        "\n",
        "  def predict(self, X,Y ):\n",
        "      output =  self.forward_propagation(X)\n",
        "      out_class=(np.argmax(output,axis=0))\n",
        "      accuracy = round(self.accuracy_score(X, Y))\n",
        "      return accuracy , out_class\n",
        "  \n",
        "  def accuracy_score(self, X, Y):\n",
        "    pred_labels = np.argmax(self.forward_propagation(X), axis=0)\n",
        "    return 100*(np.sum(pred_labels == Y) / len(Y))\n",
        "\n",
        "  \n",
        "  def predict_one_hot_encoded(self, X,Y ):\n",
        "      output =  self.forward_propagation(X)\n",
        "      accuracy = round(self.accuracy_score(X, Y))\n",
        "      return output,accuracy\n",
        "\n",
        "\n",
        "  def train(self,X_train,y_train,X_val ,y_val ,learning_rate,epochs, optimiser='gd',batch_size = 64,lambd=0.0005,WandB=False): \n",
        "\n",
        "      update_w = np.zeros(np.array(self.weights).shape)\n",
        "      update_b = np.zeros(np.array(self.biases).shape)\n",
        "      update_w_i = np.zeros(np.array(self.weights).shape)\n",
        "      update_b_i = np.zeros(np.array(self.biases).shape)\n",
        "      \n",
        "      vw_i, vb_i, m_t_i, v_t_i, m_b_i, v_b_i=0.0,0.0,0.0,0.0,0.0,0.0\n",
        "      m_t, v_t, m_hat_w, v_hat_w, m_b,v_b,m_hat_b,v_hat_b = 0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 \n",
        "\n",
        "      #below values taken from lecture slides for stability\n",
        "      eps = 1e-8\n",
        "      gamma = 0.9\n",
        "      beta = 0.999\n",
        "      beta1 = 0.9\n",
        "      beta2 = 0.999\n",
        "      train_accuracy, val_accuracy, training_loss ,validation_loss = [] ,[] ,[] ,[]\n",
        "      \n",
        "      \n",
        "      for i in tqdm(range(epochs)):\n",
        "        for batch in range(0, X_train.shape[1], batch_size):\n",
        "\n",
        "          batch_images =  X_train[:,batch:batch+batch_size]\n",
        "          batch_output =  Y_train_encoded[:,batch:batch+batch_size]\n",
        "          output =  self.forward_propagation(batch_images)\n",
        "          g_weights,g_biases = self.back_propagation(batch_images,batch_output)\n",
        "          if optimiser == 'gd':\n",
        "              self.weights,self.biases=self.stochastic_gradient_descent(self.weights,self.biases,g_weights,g_biases,learning_rate,lambd)\n",
        "          \n",
        "          if optimiser == 'mgd':\n",
        "              self.weights,self.biases,update_w,update_b=self.momentum_based_gradient_descent(self.weights,self.biases,g_weights,g_biases,update_w_i,update_b_i,learning_rate,lambd,gamma)\n",
        "              update_w_i = update_w\n",
        "              update_b_i = update_b\n",
        "\n",
        "          if optimiser == 'ngd':\n",
        "              w_t,b_t,update_w,update_b=self.nesterov_accelerated_gradient_descent(self.weights,self.biases,g_weights,g_biases,learning_rate,lambd,gamma,update_w_i,update_b_i,batch_images,batch_output)\n",
        "              self.weights = w_t\n",
        "              self.biases = b_t\n",
        "              update_w_i = update_w\n",
        "              update_b_i = update_b\n",
        "\n",
        "          if optimiser == 'rmsprop': \n",
        "              self.weights,self.biases,vw,vb= self.rmsprop(self.weights,self.biases,g_weights,g_biases,learning_rate,lambd,beta,eps,vw_i,vb_i)\n",
        "              vw_i=vw\n",
        "              vb_i=vb\n",
        "\n",
        "          if optimiser == 'adam':\n",
        "              self.weights,self.biases,m_t,v_t,m_b,v_b=self.adam(self.weights,self.biases,g_weights,g_biases,beta1,beta2,learning_rate, m_t_i, v_t_i, m_b_i, v_b_i,eps,i,lambd)\n",
        "              m_t_i=m_t\n",
        "              v_t_i=v_t\n",
        "              m_b_i=m_b\n",
        "              v_b_i=v_b\n",
        "          \n",
        "          if optimiser == 'nadam':\n",
        "              self.weights,self.biases,m_t,v_t,m_b,v_b=self.nadam(self.weights,self.biases,g_weights,g_biases,beta1,beta2,learning_rate, m_t_i,v_t_i, m_b_i,v_b_i, eps, i, lambd)\n",
        "              m_t_i=m_t\n",
        "              v_t_i=v_t\n",
        "              m_b_i=m_b\n",
        "              v_b_i=v_b                         \n",
        "\n",
        "        #Calculating accuracies \n",
        "        acc1=self.accuracy_score(X_train,y_train)\n",
        "        train_accuracy.append(acc1)\n",
        "  \n",
        "        acc2=self.accuracy_score(X_val,y_val)\n",
        "        val_accuracy.append(acc2)\n",
        "\n",
        "        predicted_train = self.forward_propagation(X_train)\n",
        "        predicted_val = self.forward_propagation(X_val)\n",
        "\n",
        "        a =self.weights[1:len(self.hidden_layer)]\n",
        "        b = np.sum([(np.sum((a[i]**2).reshape(1,-1))) for i in range(len(a))])#this is done to update loss function for weight decay problem \n",
        "        \n",
        "        train_loss= loss_function[self.loss_func](Y_train_encoded, y_train, predicted_train, lambd,b )\n",
        "        val_loss= loss_function[self.loss_func](Y_val_encoded, y_val, predicted_val, lambd,b )\n",
        "\n",
        "        training_loss.append(train_loss)\n",
        "        validation_loss.append(val_loss)\n",
        "        #print(training_loss)\n",
        "        #print(val_loss)\n",
        "\n",
        "        print('Epoch {} : training_accuracy = {:.2f}, training_loss = {:.4f},Validation accuracy = {:.2f},Validation loss = {:.4f}'.format(i+1,acc1,train_loss, acc2,val_loss))\n",
        "        if WandB:\n",
        "          wandb.log({\"val_accuracy\": acc2,\"accuracy\": acc1,\"steps\":epochs,\"train_loss\":train_loss,\"val_loss\":val_loss},)\n",
        "\n",
        "      \n",
        "      return train_accuracy,val_accuracy,training_loss, validation_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "4QHPBlX6Q5gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2 SOLVED BELOW**\n",
        "Forward propagation"
      ],
      "metadata": {
        "id": "UttSHvtTj7M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q2    = NeuralNet(X_train.shape[0],[64,64,64], 10,'xavier','sigmoid','cross_entropy')\n",
        "A=q2.forward_propagation(X_train)\n",
        "A=A.T\n",
        "print(A[0])\n",
        "print((Y_train_encoded.T)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPh4aKLwnXfT",
        "outputId": "2b8cd4d0-b9ff-4206-a5fb-63f8d01fa913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0492696  0.72493824 0.0113433  0.01699098 0.00191964 0.00223488\n",
            " 0.10496657 0.06102233 0.00334236 0.02397209]\n",
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    }
  ]
}